{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOx6pUG3lyNa0J9BLPkoPkf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HimAir10/Pw-skillsAssignment/blob/main/Ensemble_Techniques_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Theory Questions"
      ],
      "metadata": {
        "id": "-v3IjUYpzR2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it."
      ],
      "metadata": {
        "id": "ym5xkpSQzRza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Ensemble Learning is a machine learning technique where multiple individual models (often called base learners or weak learners) are combined to build a stronger and more robust model. The key idea is that a group of models, when aggregated properly, often performs better than any single model alone. This works because different models may capture different aspects or patterns in the data, and combining their predictions helps reduce variance, bias, or both. Common ensemble methods include Bagging (Bootstrap Aggregating), Boosting, and Stacking. In essence, ensemble learning leverages the “wisdom of the crowd” to make more accurate and stable predictions."
      ],
      "metadata": {
        "id": "agyJPhxTzRxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: What is the difference between Bagging and Boosting?"
      ],
      "metadata": {
        "id": "eieoraCZzRvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "Bagging (Bootstrap Aggregating):\n",
        "Bagging trains multiple base learners independently on different bootstrap samples of the training data. The predictions are then combined (e.g., majority voting for classification, averaging for regression). Its main strength lies in reducing variance and avoiding overfitting. Random Forest is a popular example.\n",
        "\n",
        "Boosting:\n",
        "Boosting builds base learners sequentially, where each new model focuses on correcting the errors made by the previous ones. It assigns higher weights to misclassified samples to force the next learner to pay more attention to them. Boosting reduces both bias and variance and produces strong models, but it is more prone to overfitting if not tuned properly. Examples include AdaBoost, Gradient Boosting, and XGBoost."
      ],
      "metadata": {
        "id": "NqleEh5TzRsn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?"
      ],
      "metadata": {
        "id": "96KANsHqzkhn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Bootstrap sampling is a technique where we create multiple training datasets by randomly sampling with replacement from the original dataset. Each bootstrap sample has the same size as the original dataset but may contain duplicate observations. In Bagging methods like Random Forest, bootstrap sampling ensures that each base learner is trained on slightly different data subsets, introducing diversity among models. This diversity is crucial because averaging the outputs of diverse models helps reduce overfitting and variance, leading to more stable and accurate predictions."
      ],
      "metadata": {
        "id": "OlqTHv-RzkfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?"
      ],
      "metadata": {
        "id": "ciytbibtzkc8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Out-of-Bag (OOB) samples are the data points that are not included in a particular bootstrap sample. Since about one-third of the data is left out in each bootstrap, these OOB samples can act as a natural validation set for evaluating the performance of that specific base learner. The OOB score is the average prediction accuracy on all OOB samples across the ensemble. It provides a reliable estimate of the model’s generalization performance without needing a separate validation dataset, making it especially useful when data is limited."
      ],
      "metadata": {
        "id": "a_-2rqwBzkaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest."
      ],
      "metadata": {
        "id": "aU7GwZyBzkYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "In a single Decision Tree, feature importance is determined by how much each feature contributes to reducing impurity (e.g., Gini index or entropy) at its split points. However, a single tree can be highly sensitive to noise and may assign exaggerated importance to certain features.\n",
        "\n",
        "In a Random Forest, feature importance is averaged over many trees, making it more robust and reliable. Since different trees are trained on different bootstrap samples and random feature subsets, Random Forest captures the overall significance of features across multiple decision boundaries, reducing bias and variance in feature importance estimation."
      ],
      "metadata": {
        "id": "PAAg_3hYzkWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Questions"
      ],
      "metadata": {
        "id": "JjjOxtI402wU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 6: Python Program – Random Forest Feature Importance"
      ],
      "metadata": {
        "id": "aSqJ-YHzzkUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importance\n",
        "importances = pd.Series(rf.feature_importances_, index=feature_names)\n",
        "top_features = importances.sort_values(ascending=False).head(5)\n",
        "\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(top_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GZuJSkDz7tM",
        "outputId": "64bb967e-ee5b-4030-fd05-0ff6fd2e1ef0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7: Python Program – Bagging Classifier vs. Decision Tree on Iris"
      ],
      "metadata": {
        "id": "Umt4M6LOzkSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_acc = accuracy_score(y_test, dt.predict(X_test))\n",
        "\n",
        "# Bagging Classifier\n",
        "bag = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "\n",
        "bag.fit(X_train, y_train)\n",
        "bag_acc = accuracy_score(y_test, bag.predict(X_test))\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEeBTd_o0Bxe",
        "outputId": "97f6dc03-d10f-4bad-af32-00ee35ea2629"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8: Python Program – Random Forest with GridSearchCV"
      ],
      "metadata": {
        "id": "9_nbHcGFzkP7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcpA0TztzL0W",
        "outputId": "309af640-e98d-4e6c-cd15-8657716d0e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 5, 'n_estimators': 50}\n",
            "Final Accuracy: 0.972027972027972\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Hyperparameter tuning\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7, None],\n",
        "    'n_estimators': [50, 100, 200]\n",
        "}\n",
        "grid = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Results\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Final Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Python Program – Bagging Regressor vs. Random Forest Regressor"
      ],
      "metadata": {
        "id": "n38TswA70XII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Bagging Regressor\n",
        "bag = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "\n",
        "bag.fit(X_train, y_train)\n",
        "bag_mse = mean_squared_error(y_test, bag.predict(X_test))\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_mse = mean_squared_error(y_test, rf.predict(X_test))\n",
        "\n",
        "print(\"Bagging Regressor MSE:\", bag_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blt-B5WC0Tok",
        "outputId": "94f9f322-9da3-4bd6-d8f6-a1f905b89ec0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.2582477439355284\n",
            "Random Forest Regressor MSE: 0.2542358390056568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10: Real-world Ensemble Approach (Loan Default Prediction)"
      ],
      "metadata": {
        "id": "tk8tOKte0cpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "Choosing between Bagging and Boosting:\n",
        "Since predicting loan defaults is a high-stakes classification problem where false negatives (predicting non-default when actually defaulting) can be costly, I would choose Boosting (e.g., XGBoost, LightGBM) because it reduces both bias and variance and usually achieves higher accuracy on imbalanced datasets.\n",
        "\n",
        "Handling Overfitting:\n",
        "I would use techniques like early stopping, regularization parameters (L1/L2 penalties, learning rate tuning), and cross-validation to ensure the model does not overfit the training data.\n",
        "\n",
        "Selecting Base Models:\n",
        "Decision Trees are the most common base learners for both Bagging and Boosting. For financial data, I would start with shallow trees (depth 3–6) to capture non-linear patterns while avoiding over-complexity.\n",
        "\n",
        "Evaluating Performance using Cross-Validation:\n",
        "I would perform stratified k-fold cross-validation to ensure balanced class representation across folds. Evaluation metrics would include AUC-ROC, Precision-Recall, and F1-score since the dataset might be imbalanced.\n",
        "\n",
        "Justifying Ensemble Learning in Decision-Making:\n",
        "Ensemble models provide more robust and stable predictions compared to a single model. In a financial context, they help minimize risks by capturing diverse patterns in customer behavior and transaction history. This improves loan approval accuracy, reduces default risk, and ensures fairer decision-making across different customer segments."
      ],
      "metadata": {
        "id": "45K8as0e0nOW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fnROeKHm0Zp4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}